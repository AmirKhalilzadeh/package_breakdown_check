{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "This notebook aimes to automate the process of extracting and analyzing import statements from multiple Jupyter Notebooks within a GitHub repository. The main idea is to collect all the import statements from the code cells of these notebooks to check for dependency breakdowns and version conflicts upon an update in the virtual environment.\n",
    "\n",
    "### Outline\n",
    "\n",
    "- **GitHub authentication and repository access**\n",
    "\n",
    "- **Extracting import statements**\n",
    "\n",
    "- **Processing import statements and validation**:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from github import Github\n",
    "\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GitHub Authenticaton\n",
    "\n",
    "Accessed the specified repository and retrieved all notebook files within it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the GitHub token from environment variable\n",
    "token = os.getenv(\"GITHUB_TOKEN_prj_pkg\")\n",
    "\n",
    "# authenticate to github\n",
    "g = Github(token)\n",
    "\n",
    "# get the authenticated user\n",
    "user = g.get_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated as: AmirKhalilzadeh\n",
      "Found repository: epfl-exts/adsml-ibex\n"
     ]
    }
   ],
   "source": [
    "# print the authenticated user's login\n",
    "print(f\"Authenticated as: {user.login}\")\n",
    "\n",
    "# define the organization and repository name\n",
    "org_name = \"epfl-exts\"  # Replace with the actual organization name\n",
    "repo_name = \"adsml-ibex\"  # Replace with the actual repository name\n",
    "\n",
    "# check if you find the repository\n",
    "repo = g.get_repo(f\"{org_name}/{repo_name}\")\n",
    "print(f\"Found repository: {repo.full_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting import statements\n",
    "\n",
    "Define functions to recursively get all notebook files and extract import statements from the code cells. Then collect all import statements from the notebooks and stored them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recursively get all notebook files in a repository\n",
    "def get_notebook_files(repo, path=\"\"):\n",
    "    contents = repo.get_contents(path)\n",
    "    notebooks = []\n",
    "    for content_file in contents:\n",
    "        if content_file.type == \"dir\":\n",
    "            notebooks.extend(get_notebook_files(repo, content_file.path))\n",
    "        elif content_file.name == \"notebook.ipynb\":\n",
    "            notebooks.append(content_file.path)\n",
    "    return notebooks\n",
    "\n",
    "\n",
    "# extract import statements from a notebook\n",
    "def extract_import_statements(notebook_content):\n",
    "    import_statements = []\n",
    "    for cell in notebook_content[\"cells\"]:\n",
    "        if cell[\"cell_type\"] == \"code\":\n",
    "            for line in cell[\"source\"]:\n",
    "                if line.startswith(\"import\") or line.startswith(\"from\"):\n",
    "                    import_statements.append(line.strip())\n",
    "    return import_statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'notebook.ipynb'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "346"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all notebook files in the repository\n",
    "notebook_files = get_notebook_files(repo)\n",
    "\n",
    "# Extract the base names of the notebook files\n",
    "notebook_names = [os.path.basename(file) for file in notebook_files]\n",
    "\n",
    "# Print the unique names\n",
    "display(set(notebook_names))\n",
    "len(notebook_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'from IPython.display import display',\n",
       " 'from PIL import Image',\n",
       " 'from bs4 import BeautifulSoup',\n",
       " 'from collections import Counter',\n",
       " 'from components import QuizzComponent',\n",
       " 'from gensim.models import Phrases',\n",
       " 'from itertools import product',\n",
       " 'from matplotlib import pyplot as plt',\n",
       " 'from matplotlib.patches import Ellipse',\n",
       " 'from mpl_toolkits.mplot3d import Axes3D',\n",
       " 'from nltk.corpus import stopwords',\n",
       " 'from nltk.stem.porter import PorterStemmer',\n",
       " 'from nltk.tokenize import TreebankWordTokenizer',\n",
       " 'from nltk.tokenize import sent_tokenize',\n",
       " 'from pandas import json_normalize',\n",
       " 'from pandas.plotting import autocorrelation_plot, lag_plot',\n",
       " 'from pandas.plotting import lag_plot',\n",
       " 'from pandas.tseries.offsets import *',\n",
       " 'from scipy import stats',\n",
       " 'from scipy.io import wavfile',\n",
       " 'from scipy.linalg import lstsq',\n",
       " 'from scipy.signal import hilbert',\n",
       " 'from scipy.stats import skew, kurtosis',\n",
       " 'from scipy.stats import zscore',\n",
       " 'from sklearn import datasets',\n",
       " 'from sklearn import set_config',\n",
       " 'from sklearn.base import BaseEstimator, ClassifierMixin',\n",
       " 'from sklearn.base import BaseEstimator, TransformerMixin',\n",
       " 'from sklearn.base import RegressorMixin, clone',\n",
       " 'from sklearn.base import clone',\n",
       " 'from sklearn.cluster import KMeans',\n",
       " 'from sklearn.compose import ColumnTransformer',\n",
       " 'from sklearn.compose import TransformedTargetRegressor',\n",
       " 'from sklearn.datasets import fetch_openml',\n",
       " 'from sklearn.datasets import load_boston',\n",
       " 'from sklearn.datasets import load_files',\n",
       " 'from sklearn.datasets import make_blobs',\n",
       " 'from sklearn.datasets import make_circles',\n",
       " 'from sklearn.datasets import make_classification',\n",
       " 'from sklearn.decomposition import PCA',\n",
       " 'from sklearn.dummy import DummyClassifier',\n",
       " 'from sklearn.dummy import DummyRegressor',\n",
       " 'from sklearn.ensemble import IsolationForest',\n",
       " 'from sklearn.ensemble import RandomForestClassifier',\n",
       " 'from sklearn.ensemble import RandomForestRegressor',\n",
       " 'from sklearn.feature_extraction.text import CountVectorizer',\n",
       " 'from sklearn.feature_extraction.text import TfidfVectorizer',\n",
       " 'from sklearn.feature_selection import SelectKBest, chi2',\n",
       " 'from sklearn.impute import SimpleImputer',\n",
       " 'from sklearn.linear_model import HuberRegressor',\n",
       " 'from sklearn.linear_model import Lasso',\n",
       " 'from sklearn.linear_model import LinearRegression',\n",
       " 'from sklearn.linear_model import LogisticRegression',\n",
       " 'from sklearn.linear_model import LogisticRegressionCV',\n",
       " 'from sklearn.linear_model import Ridge',\n",
       " 'from sklearn.linear_model import SGDRegressor',\n",
       " 'from sklearn.metrics import ConfusionMatrixDisplay',\n",
       " 'from sklearn.metrics import classification_report',\n",
       " 'from sklearn.metrics import confusion_matrix',\n",
       " 'from sklearn.metrics import f1_score',\n",
       " 'from sklearn.metrics import mean_absolute_error as MAE',\n",
       " 'from sklearn.metrics import mean_squared_error',\n",
       " 'from sklearn.metrics import mean_squared_error as MSE',\n",
       " 'from sklearn.metrics import mean_squared_error as mse',\n",
       " 'from sklearn.metrics import precision_score',\n",
       " 'from sklearn.metrics import r2_score',\n",
       " 'from sklearn.metrics import recall_score',\n",
       " 'from sklearn.metrics import roc_auc_score',\n",
       " 'from sklearn.metrics import roc_curve',\n",
       " 'from sklearn.model_selection import GridSearchCV',\n",
       " 'from sklearn.model_selection import KFold',\n",
       " 'from sklearn.model_selection import ParameterGrid',\n",
       " 'from sklearn.model_selection import ShuffleSplit',\n",
       " 'from sklearn.model_selection import TimeSeriesSplit',\n",
       " 'from sklearn.model_selection import cross_validate',\n",
       " 'from sklearn.model_selection import train_test_split',\n",
       " 'from sklearn.neighbors import KNeighborsClassifier',\n",
       " 'from sklearn.neighbors import NearestNeighbors',\n",
       " 'from sklearn.neural_network import MLPClassifier',\n",
       " 'from sklearn.pipeline import Pipeline',\n",
       " 'from sklearn.pipeline import make_pipeline',\n",
       " 'from sklearn.preprocessing import FunctionTransformer',\n",
       " 'from sklearn.preprocessing import LabelEncoder',\n",
       " 'from sklearn.preprocessing import MinMaxScaler',\n",
       " 'from sklearn.preprocessing import OneHotEncoder',\n",
       " 'from sklearn.preprocessing import OrdinalEncoder',\n",
       " 'from sklearn.preprocessing import PolynomialFeatures',\n",
       " 'from sklearn.preprocessing import PowerTransformer',\n",
       " 'from sklearn.preprocessing import QuantileTransformer',\n",
       " 'from sklearn.preprocessing import RobustScaler',\n",
       " 'from sklearn.preprocessing import StandardScaler',\n",
       " 'from sklearn.preprocessing import StandardScaler, FunctionTransformer',\n",
       " 'from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler',\n",
       " 'from sklearn.preprocessing import scale',\n",
       " 'from sklearn.svm import LinearSVC',\n",
       " 'from sklearn.svm import SVC',\n",
       " 'from sklearn.tree import DecisionTreeClassifier',\n",
       " 'from sklearn.tree import plot_tree',\n",
       " 'from statsmodels.tsa.deterministic import DeterministicProcess',\n",
       " 'from statsmodels.tsa.seasonal import STL',\n",
       " 'from statsmodels.tsa.stattools import adfuller',\n",
       " 'from statsmodels.tsa.stattools import kpss',\n",
       " 'from tensorflow import keras',\n",
       " 'from tensorflow.keras import Sequential',\n",
       " 'from tensorflow.keras import activations',\n",
       " 'from tensorflow.keras import backend as K',\n",
       " 'from tensorflow.keras import initializers',\n",
       " 'from tensorflow.keras import losses',\n",
       " 'from tensorflow.keras import metrics',\n",
       " 'from tensorflow.keras import optimizers',\n",
       " 'from tensorflow.keras.applications.xception import decode_predictions',\n",
       " 'from tensorflow.keras.datasets.mnist import load_data',\n",
       " 'from tensorflow.keras.layers import (Conv2D, BatchNormalization, Dropout,',\n",
       " 'from tensorflow.keras.layers import Dense',\n",
       " 'from tensorflow.keras.preprocessing.image import ImageDataGenerator',\n",
       " 'from zipfile import ZipFile',\n",
       " 'import IPython.display as ipd',\n",
       " 'import PIL.Image as Image',\n",
       " 'import json',\n",
       " 'import librosa',\n",
       " 'import librosa.display',\n",
       " 'import matplotlib.patheffects as path_effects',\n",
       " 'import matplotlib.pyplot as plt',\n",
       " 'import matplotlib.pyplot as plt_exts',\n",
       " 'import missingno as msno',\n",
       " 'import nltk',\n",
       " 'import noisereduce as nr',\n",
       " 'import numpy',\n",
       " 'import numpy as np',\n",
       " 'import numpy.polynomial.polynomial as poly',\n",
       " 'import os',\n",
       " 'import os, sys',\n",
       " 'import pandas as pd',\n",
       " 'import pickle',\n",
       " 'import random',\n",
       " 'import re',\n",
       " 'import requests',\n",
       " 'import scipy',\n",
       " 'import scipy.stats as stats',\n",
       " 'import seaborn',\n",
       " 'import seaborn as sns',\n",
       " 'import sqlite3',\n",
       " 'import statsmodels.api as sm',\n",
       " 'import string',\n",
       " 'import tensorflow as tf',\n",
       " 'import tensorflow.keras as keras',\n",
       " 'import tensorflow_hub as hub',\n",
       " 'import tensorflow_text',\n",
       " 'import w, sys',\n",
       " 'import warnings'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect all import statements\n",
    "all_import_statements = []\n",
    "for notebook_file in notebook_files:\n",
    "    file_content = repo.get_contents(notebook_file).decoded_content.decode(\"utf-8\")\n",
    "    notebook_content = json.loads(file_content)\n",
    "    all_import_statements.extend(extract_import_statements(notebook_content))\n",
    "\n",
    "set(all_import_statements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(len(set(all_import_statements)))\n",
    "\n",
    "# Remove specific import statements\n",
    "all_import_statements = [\n",
    "    stmt\n",
    "    for stmt in all_import_statements\n",
    "    if stmt != \"from components import QuizzComponent\" and stmt != \"import w, sys\"\n",
    "]\n",
    "\n",
    "# Display the updated list\n",
    "len(set(all_import_statements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list to a set to get unique import statements\n",
    "unique_import_statements = set(all_import_statements)\n",
    "\n",
    "# Save the unique import statements to a file\n",
    "with open(\"unique_import_statements.txt\", \"w\") as file:\n",
    "    for statement in unique_import_statements:\n",
    "        file.write(statement + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n",
    "Activate the environment and check whether the import statements are fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load unique_import_statements.txt file\n",
    "with open(\"unique_import_statements.txt\", \"r\") as file:\n",
    "    unique_import_statements = file.readlines()\n",
    "\n",
    "len(unique_import_statements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if import statements work\n",
    "def check_imports(import_statements):\n",
    "\tfor statement in import_statements:\n",
    "\t\ttry:\n",
    "\t\t\texec(statement)\n",
    "\t\t\tprint(f\"Successfully imported: {statement}\")\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(f\"Failed to import: {statement} - Error: {e}\")\n",
    "\n",
    "# check all collected import statements\n",
    "check_imports(set(unique_import_statements))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are two failures:\n",
    "# Failed to import: import tensorflow_text\n",
    "#  - Error: No module named 'tensorflow_text'\n",
    "\n",
    "#  Failed to import: from tensorflow.keras.layers import (Conv2D, BatchNormalization, Dropout,\n",
    "#  - Error: unexpected EOF while parsing (<string>, line 1)\n",
    "\n",
    "# under the current env we have the following failure too:\n",
    "# Failed to import: import noisereduce as nr\n",
    "#  - Error: No module named 'torch'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
